{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573422ea",
   "metadata": {},
   "source": [
    "# Active Image Alignment\n",
    "For most use cases, each band of a multispectral capture must be aligned with the other bands in order to create meaningful data. In this tutorial, we show how to align the band to each other using open source OpenCV utilities.\n",
    "\n",
    "Image alignment allows the combination of images into true-color (RGB) and false color (such as CIR) composites, useful for scouting using single images as well as for display and management uses. In addition to composite images, alignment allows the calculation of pixel-accurate indices such as NDVI or NDRE at the single image level which can be very useful for applications like plant counting and coverage estimations, where mosaicing artifacts may otherwise skew analysis results.\n",
    "\n",
    "The image alignment method described below tends to work well on images with abundant image features, or areas of significant contrast. Cars, buildings, parking lots, and roads tend to provide the best results. This approach may not work well on images which contain few features or very repetitive features, such as full canopy row crops or fields of repetitive small crops such lettuce or strawberries. We will disscuss more about the advantages and disadvantages of these methods below.\n",
    "\n",
    "The functions behind this alignment process can work with most versions of RedEdge and Altum firmware. They will work best with RedEdge (3,M,MX) versions above 3.2.0 which include the \"RigRelatives\" tags, and all RedEdge-P/Altum/Altum-PT imagery. These tags provide a starting point for the image transformation and can help to ensure convergence of the algorithm.\n",
    "\n",
    "# Opening Images\n",
    "As we have done in previous examples, we use the micasense.capture class to open, radiometrically correct, and visualize all the bands of a MicaSense capture.\n",
    "\n",
    "First, we'll load the `autoreload` extension. This lets us change underlying code (such as library functions) without having to reload the entire workbook and kernel. This is useful in this workbook because the cell that runs the alignment can take a long time to run, so with the `autoreload` extension we can update the code after the alignment step for analysis and visualization without needing to re-compute the alignments each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b09cd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7426068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera model: Altum-PT\n",
      "Bit depth: 12\n",
      "Camera serial number: PA01-2210071-MS\n",
      "Capture ID: Z9WKG9xZ2rL9gFGQopS0\n"
     ]
    },
    {
     "ename": "LibRawFileUnsupportedError",
     "evalue": "b'Unsupported file format or not RAW file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibRawFileUnsupportedError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m     panSharpen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m panelCap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpanelCap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpanel_albedo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m         panel_reflectance_by_band \u001b[38;5;241m=\u001b[39m panelCap\u001b[38;5;241m.\u001b[39mpanel_albedo()\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mG:\\GIT_Repo\\micasense\\imageprocessing\\micasense\\capture.py:429\u001b[0m, in \u001b[0;36mCapture.panel_albedo\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpanel_albedo\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of panel reflectance values from metadata.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpanels_in_all_expected_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    430\u001b[0m         albedos \u001b[38;5;241m=\u001b[39m [panel\u001b[38;5;241m.\u001b[39mreflectance_from_panel_serial() \u001b[38;5;28;01mfor\u001b[39;00m panel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpanels]\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m albedos:\n",
      "File \u001b[1;32mG:\\GIT_Repo\\micasense\\imageprocessing\\micasense\\capture.py:376\u001b[0m, in \u001b[0;36mCapture.panels_in_all_expected_images\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03mCheck if all expected reflectance panels are detected in the EO Images in the Capture.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m:return: True if reflectance panels are detected.\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    375\u001b[0m expected_panels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mstr\u001b[39m(img\u001b[38;5;241m.\u001b[39mband_name)\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLWIR\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages)\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_panels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m expected_panels\n",
      "File \u001b[1;32mG:\\GIT_Repo\\micasense\\imageprocessing\\micasense\\capture.py:442\u001b[0m, in \u001b[0;36mCapture.detect_panels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpanels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetected_panel_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages):\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetected_panel_count\n\u001b[1;32m--> 442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpanels \u001b[38;5;241m=\u001b[39m [Panel(img, panel_corners\u001b[38;5;241m=\u001b[39mpc) \u001b[38;5;28;01mfor\u001b[39;00m img, pc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpanelCorners)]\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetected_panel_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpanels:\n",
      "File \u001b[1;32mG:\\GIT_Repo\\micasense\\imageprocessing\\micasense\\capture.py:442\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpanels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetected_panel_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages):\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetected_panel_count\n\u001b[1;32m--> 442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpanels \u001b[38;5;241m=\u001b[39m [\u001b[43mPanel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpanel_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img, pc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpanelCorners)]\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetected_panel_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpanels:\n",
      "File \u001b[1;32mG:\\GIT_Repo\\micasense\\imageprocessing\\micasense\\panel.py:45\u001b[0m, in \u001b[0;36mPanel.__init__\u001b[1;34m(self, img, panel_corners, ignore_autocalibration)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide an image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage \u001b[38;5;241m=\u001b[39m img\n\u001b[1;32m---> 45\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradiance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmin()\n\u001b[0;32m     46\u001b[0m scale \u001b[38;5;241m=\u001b[39m (img\u001b[38;5;241m.\u001b[39mradiance()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m bias)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgray8b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(img\u001b[38;5;241m.\u001b[39mradiance()\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mG:\\GIT_Repo\\micasense\\imageprocessing\\micasense\\image.py:318\u001b[0m, in \u001b[0;36mImage.radiance\u001b[1;34m(self, force_recompute)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__radiance_image\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# get image dimensions\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m image_raw \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mband_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLWIR\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m#  get radiometric calibration factors\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     a1, a2, a3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mradiometric_cal[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mradiometric_cal[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mradiometric_cal[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mG:\\GIT_Repo\\micasense\\imageprocessing\\micasense\\image.py:225\u001b[0m, in \u001b[0;36mImage.raw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# to support 12-bit DNG files, otherwise we get \"SIFT found no features\" error\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbits_per_pixel \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m12\u001b[39m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__raw_image \u001b[38;5;241m=\u001b[39m \u001b[43mrawpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mraw_image \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__raw_image \u001b[38;5;241m=\u001b[39m rawpy\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\u001b[38;5;241m.\u001b[39mraw_image\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\micasense\\lib\\site-packages\\rawpy\\__init__.py:20\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(pathOrFile)\u001b[0m\n\u001b[0;32m     18\u001b[0m     d\u001b[38;5;241m.\u001b[39mopen_buffer(pathOrFile)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathOrFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "File \u001b[1;32mrawpy\\\\_rawpy.pyx:409\u001b[0m, in \u001b[0;36mrawpy._rawpy.RawPy.open_file\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrawpy\\\\_rawpy.pyx:936\u001b[0m, in \u001b[0;36mrawpy._rawpy.RawPy.handle_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mLibRawFileUnsupportedError\u001b[0m: b'Unsupported file format or not RAW file'"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import micasense.capture as capture\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.facecolor\"] = \"w\"\n",
    "\n",
    "panelNames = None\n",
    "\n",
    "# set your image paths here. See more here: https://docs.python.org/3/library/pathlib.html\n",
    "# if using Windows, you need to an an \"r\" to the path like this: Path(r\"C:\\Files\") \n",
    "\n",
    "# imagePath = Path(\"./data/REDEDGE-MX-DUAL\")\n",
    "\n",
    "# # these will return lists of image paths as strings \n",
    "# imageNames = list(imagePath.glob('IMG_0431_*.tif'))\n",
    "# imageNames = [x.as_posix() for x in imageNames]\n",
    "\n",
    "# panelNames = list(imagePath.glob('IMG_0000_*.tif'))\n",
    "# panelNames = [x.as_posix() for x in panelNames]\n",
    "\n",
    "# imagePath = Path(\"./data/REDEDGE-MX\")\n",
    "\n",
    "# # these will return lists of image paths as strings \n",
    "# imageNames = list(imagePath.glob('IMG_0020_*.tif'))\n",
    "# imageNames = [x.as_posix() for x in imageNames]\n",
    "\n",
    "# panelNames = list(imagePath.glob('IMG_0001_*.tif'))\n",
    "# panelNames = [x.as_posix() for x in panelNames]\n",
    "\n",
    "# imagePath = Path(\"./data/ALTUM\")\n",
    "\n",
    "# # these will return lists of image paths as strings \n",
    "# imageNames = list(imagePath.glob('IMG_0021_*.tif'))\n",
    "# imageNames = [x.as_posix() for x in imageNames]\n",
    "\n",
    "# panelNames = list(imagePath.glob('IMG_0000_*.tif'))\n",
    "# panelNames = [x.as_posix() for x in panelNames]\n",
    "\n",
    "# imagePath = Path(\"./data/REDEDGE-P\")\n",
    "\n",
    "# # these will return lists of image paths as strings \n",
    "# imageNames = list(imagePath.glob('IMG_0011_*.tif'))\n",
    "# imageNames = [x.as_posix() for x in imageNames]\n",
    "\n",
    "# panelNames = list(imagePath.glob('IMG_0000_*.tif'))\n",
    "# panelNames = [x.as_posix() for x in panelNames]\n",
    "\n",
    "imagePath = Path(\"./data/ALTUM-PT\")\n",
    "\n",
    "# these will return lists of image paths as strings \n",
    "imageNames = list(imagePath.glob('IMG_0010_*.tif'))\n",
    "imageNames = [x.as_posix() for x in imageNames]\n",
    "\n",
    "panelNames = list(imagePath.glob('IMG_0000_*.tif'))\n",
    "panelNames = [x.as_posix() for x in panelNames]\n",
    "\n",
    "\n",
    "if panelNames is not None:\n",
    "    panelCap = capture.Capture.from_filelist(panelNames)\n",
    "else:\n",
    "    panelCap = None\n",
    "\n",
    "thecapture = capture.Capture.from_filelist(imageNames)\n",
    "\n",
    "# get camera model for future use \n",
    "cam_model = thecapture.camera_model\n",
    "# if this is a multicamera system like the RedEdge-MX Dual,\n",
    "# we can combine the two serial numbers to help identify \n",
    "# this camera system later. \n",
    "if len(thecapture.camera_serials) > 1:\n",
    "    cam_serial = \"_\".join(thecapture.camera_serials)\n",
    "    print(cam_serial)\n",
    "else:\n",
    "    cam_serial = thecapture.camera_serial\n",
    "    \n",
    "print(\"Camera model:\",cam_model)\n",
    "print(\"Bit depth:\", thecapture.bits_per_pixel)\n",
    "print(\"Camera serial number:\", cam_serial)\n",
    "print(\"Capture ID:\",thecapture.uuid)\n",
    "\n",
    "# determine if this sensor has a panchromatic band \n",
    "if cam_model == 'RedEdge-P' or cam_model == 'Altum-PT':\n",
    "    panchroCam = True\n",
    "else:\n",
    "    panchroCam = False\n",
    "    panSharpen = False \n",
    "\n",
    "if panelCap is not None:\n",
    "    if panelCap.panel_albedo() is not None:\n",
    "        panel_reflectance_by_band = panelCap.panel_albedo()\n",
    "    else:\n",
    "        panel_reflectance_by_band = [0.49]*len(thecapture.eo_band_names()) #RedEdge band_index order\n",
    "    panel_irradiance = panelCap.panel_irradiance(panel_reflectance_by_band)  \n",
    "    irradiance_list = panelCap.panel_irradiance(panel_reflectance_by_band) + [0] # add to account for uncalibrated LWIR band, if applicable\n",
    "    img_type = \"reflectance\"\n",
    "    thecapture.plot_undistorted_reflectance(panel_irradiance)\n",
    "else:\n",
    "    if thecapture.dls_present():\n",
    "        img_type='reflectance'\n",
    "        irradiance_list = thecapture.dls_irradiance() + [0]\n",
    "        thecapture.plot_undistorted_reflectance(thecapture.dls_irradiance())\n",
    "    else:\n",
    "        img_type = \"radiance\"\n",
    "        thecapture.plot_undistorted_radiance() \n",
    "        irradiance_list = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5a774",
   "metadata": {},
   "source": [
    "# Check for existing warp matrices \n",
    "If we have already successfully aligned captures from this specific camera, we can typically save some time and use the alignment warp matrices for other captures from the same camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35de1be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing warp matrices found. Create them later in the notebook.\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import ProjectiveTransform\n",
    "import numpy as np\n",
    "\n",
    "if panchroCam:\n",
    "    warp_matrices_filename = cam_serial + \"_warp_matrices_SIFT.npy\"\n",
    "else:\n",
    "    warp_matrices_filename = cam_serial + \"_warp_matrices_opencv.npy\"\n",
    "\n",
    "if Path('./' + warp_matrices_filename).is_file():\n",
    "    print(\"Found existing warp matrices for camera\", cam_serial)\n",
    "    load_warp_matrices = np.load(warp_matrices_filename, allow_pickle=True)\n",
    "    loaded_warp_matrices = []\n",
    "    for matrix in load_warp_matrices: \n",
    "        if panchroCam:\n",
    "            transform = ProjectiveTransform(matrix=matrix.astype('float64'))\n",
    "            loaded_warp_matrices.append(transform)\n",
    "        else:\n",
    "            loaded_warp_matrices.append(matrix.astype('float32'))\n",
    "    print(\"Warp matrices successfully loaded.\")\n",
    "\n",
    "    if panchroCam:\n",
    "        warp_matrices_SIFT = loaded_warp_matrices\n",
    "    else:\n",
    "        warp_matrices = loaded_warp_matrices\n",
    "else:\n",
    "    print(\"No existing warp matrices found. Create them later in the notebook.\")\n",
    "    warp_matrices_SIFT = False\n",
    "    warp_matrices = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185627ff",
   "metadata": {},
   "source": [
    "# Unwarp and Align (OpenCV method for RedEdge3/M/MX/Dual and original Altum)\n",
    "Alignment is a three-step process:\n",
    "\n",
    "Images are unwarped using the built-in lens calibration\n",
    "A transformation is found to align each band to a common band\n",
    "The aligned images are combined and cropped, removing pixels which don't overlap in all bands.\n",
    "We provide utilities to find the alignment transformations within a single capture. Our experience shows that once a good alignment transformation is found, it tends to be stable over a flight and, in most cases, over many flights. The transformation may change if the camera undergoes a shock event (such as a hard landing or drop) or if the temperature changes substantially between flights. In these events, a new transformation may need to be found.\n",
    "\n",
    "Further, since this approach finds a 2-dimensional (affine) transformation between images, it won't work when the parallax between bands results in a 3-dimensional depth field. This can happen if very close to the target or when targets are visible at significantly different ranges, such as a nearby tree or building against a background much farther way. In these cases, it will be necessary to use photogrammetry techniques to find a 3-dimensional mapping between images.\n",
    "\n",
    "For best alignment results it's good to select a capture which has features which visible in all bands. Man-made objects such as cars, roads, and buildings tend to work very well, while captures of only repeating crop rows tend to work poorly. Remember, once a good transformation has been found for flight, it can be generally be applied across all of the images.\n",
    "\n",
    "It's also good to use an image for alignment which is taken near the same level above ground as the rest of the flights. Above approximately 35m AGL, the alignment will be consistent. However, if images taken closer to the ground are used, such as panel images, the same alignment transformation will not work for the flight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e154cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import micasense.imageutils as imageutils\n",
    "import micasense.plotutils as plotutils\n",
    "\n",
    "# We use a different alignment method for RedEdge-P and Altum-PT, \n",
    "# so if the imagery is from this kind of camera, skip this step \n",
    "if not panchroCam:\n",
    "    st = time.time()\n",
    "    # set to True if you'd like to ignore existing warp matrices and create new ones\n",
    "    regenerate = True    \n",
    "    pyramid_levels = 0 # for images with RigRelatives, setting this to 0 or 1 may improve alignment\n",
    "    max_alignment_iterations = 10\n",
    "\n",
    "    # match_index: \n",
    "    # for non-panchromatic cameras we want to use band 1, which is green \n",
    "    # the green band has zero rig relative offsets \n",
    "    # NOTE: These band numbers are zero-indexed \n",
    "    # special parameters for RedEdge-MX dual camera system \n",
    "    if len(thecapture.eo_band_names()) == 10:\n",
    "        print(\"is 10 band\")\n",
    "        pyramid_levels = 3\n",
    "        match_index = 4\n",
    "        max_alignment_iterations = 20\n",
    "    else: \n",
    "        match_index = 1\n",
    "\n",
    "    warp_mode = cv2.MOTION_HOMOGRAPHY # MOTION_HOMOGRAPHY or MOTION_AFFINE. For Altum images only use HOMOGRAPHY\n",
    "\n",
    "    print(\"Aligning images. Depending on settings this can take from a few seconds to many minutes\")\n",
    "    # Can potentially increase max_iterations for better results, but longer runtimes\n",
    "    if warp_matrices and not regenerate:\n",
    "        print(\"Using existing warp matrices...\")\n",
    "        try:\n",
    "            irradiance = panel_irradiance+[0]\n",
    "        except NameError:\n",
    "            irradiance = None\n",
    "    else:\n",
    "        warp_matrices, alignment_pairs = imageutils.align_capture(thecapture,\n",
    "                                                              ref_index = match_index,\n",
    "                                                              max_iterations = max_alignment_iterations,\n",
    "                                                              warp_mode = warp_mode,\n",
    "                                                              pyramid_levels = pyramid_levels)\n",
    "\n",
    "    print(\"Finished Aligning\")\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    print('Alignment time:', int(elapsed_time), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06bfdf3",
   "metadata": {},
   "source": [
    "# Crop Aligned Images and create aligned capture stack \n",
    "After finding image alignments, we may need to remove pixels around the edges which aren't present in every image in the capture. To do this we use the affine transforms found above and the image distortions from the image metadata. OpenCV provides a couple of handy helpers for this task in the cv2.undistortPoints() and cv2.transform() methods. These methods take a set of pixel coordinates and apply our undistortion matrix and our affine transform, respectively. So, just as we did when registering the images, we first apply the undistortion process to the coordinates of the image borders, then we apply the affine transformation to that result. The resulting pixel coordinates tell us where the image borders end up after this pair of transformations, and we can then crop the resultant image to these coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not panchroCam:\n",
    "    cropped_dimensions, edges = imageutils.find_crop_bounds(thecapture, warp_matrices, warp_mode=warp_mode, reference_band=match_index)\n",
    "    print(\"Cropped dimensions:\",cropped_dimensions)\n",
    "    im_aligned = thecapture.create_aligned_capture(warp_matrices=warp_matrices, motion_type=warp_mode, img_type=img_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c362623a",
   "metadata": {},
   "source": [
    "# Alignment and pan-sharpening for RedEdge-P and Altum-PT\n",
    "For older cameras we use OpenCV for capture alignment. For RedEdge-P and Altum-PT, we use SIFT (scale-invariant feature transform). We will use SIFT to create the warp matrices, then use these warp matrices to align the capture. See more here: \n",
    "https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_sift.html\n",
    "https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\n",
    "\n",
    "For sensors with a panchromatic band (Altum-PT or RedEdge-P), we may wish to create a pan-sharpened stack. This example uses a linear interpolation method for pan-sharpening.\n",
    "\n",
    "The `radiometric_pan_sharpen` function takes in the SIFT warp matrices and outputs an upsampled image stack (all bands are changed to the resolution of the panchromatic sensor) and a pan-sharpened stack. \n",
    "\n",
    "Note: it is important to choose an initial alignment image that has a lot of straight, manmade features, such as roads or buildings. Trying to align a capture that is strictly vegetation will take a long time and may not be very good. This process can take a while, depending on how feature-rich the capture is. We have seen some Altum-PT captures align after 2 minutes, and some can take upwards of 30 minutes. Be patient! Once an alignment has been found, it can be used on other captures from the same camera, and the alignment process will be much faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from micasense.imageutils import brovey_pan_sharpen,radiometric_pan_sharpen\n",
    "from skimage.transform import ProjectiveTransform\n",
    "import time\n",
    "\n",
    "if panchroCam: \n",
    "    # set to True if you'd like to ignore existing warp matrices and create new ones\n",
    "    regenerate = True\n",
    "    st = time.time()\n",
    "    if not warp_matrices_SIFT or regenerate:\n",
    "        print(\"Generating new warp matrices...\")\n",
    "        warp_matrices_SIFT = thecapture.SIFT_align_capture(min_matches = 10)\n",
    "        \n",
    "    sharpened_stack, upsampled = thecapture.radiometric_pan_sharpened_aligned_capture(warp_matrices=warp_matrices_SIFT, irradiance_list=irradiance_list, img_type=img_type)\n",
    "    \n",
    "# we can also use the Rig Relatives from the image metadata to do a quick, rudimentary alignment \n",
    "#     warp_matrices0=thecapture.get_warp_matrices(ref_index=5)\n",
    "#     sharpened_stack,upsampled = radiometric_pan_sharpen(thecapture,warp_matrices=warp_matrices0)\n",
    "\n",
    "    print(\"Pansharpened shape:\", sharpened_stack.shape)\n",
    "    print(\"Upsampled shape:\", upsampled.shape)\n",
    "    # re-assign to im_aligned to match rest of code \n",
    "    im_aligned = upsampled\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    print('Alignment and pan-sharpening time:', int(elapsed_time), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d51b1",
   "metadata": {},
   "source": [
    "# Save warp matrices\n",
    "Once an alignment for your camera has been found, it can be saved to a file for later use with this notebook. It can also be used on the Batch Alignment notebook for aligning all of the captures from an entire flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84855db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import skimage\n",
    "from skimage.transform import warp,matrix_transform,resize,FundamentalMatrixTransform,estimate_transform,ProjectiveTransform\n",
    "\n",
    "if panchroCam:\n",
    "    working_wm = warp_matrices_SIFT\n",
    "else:\n",
    "    working_wm = warp_matrices\n",
    "if not Path('./' + warp_matrices_filename).is_file() or regenerate:\n",
    "    temp_matrices = []\n",
    "    for x in working_wm:\n",
    "        if isinstance(x, numpy.ndarray):\n",
    "            temp_matrices.append(x)\n",
    "        if isinstance(x, skimage.transform._geometric.ProjectiveTransform):\n",
    "            temp_matrices.append(x.params)\n",
    "    np.save(warp_matrices_filename, np.array(temp_matrices, dtype=object), allow_pickle=True)\n",
    "    print(\"Saved to\", Path('./' + warp_matrices_filename).resolve())\n",
    "else:\n",
    "    print(\"Matrices already exist at\",Path('./' + warp_matrices_filename).resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3fb77",
   "metadata": {},
   "source": [
    "# Multispectral band histogram \n",
    "We can compare the radiance between multispectral bands, and between upsampled/pansharpened in the case of RedEdge-P and Altum-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e036b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "theColors = {'Blue': 'blue', 'Green': 'green', 'Red': 'red', \\\n",
    "             'Red edge': 'maroon', 'NIR': 'purple', 'Panchro': 'yellow', 'PanchroB': 'orange',\\\n",
    "            'Red edge-740': 'salmon', 'Red Edge': 'maroon', 'Blue-444': 'aqua', \\\n",
    "             'Green-531': 'lime', 'Red-650': 'lightcoral', 'Red edge-705':'brown'}\n",
    "\n",
    "eo_count = len(thecapture.eo_indices())\n",
    "multispec_min = np.min(np.percentile(im_aligned[:,:,1:eo_count].flatten(),0.01))\n",
    "multispec_max = np.max(np.percentile(im_aligned[:,:,1:eo_count].flatten(), 99.99))\n",
    "\n",
    "theRange = (multispec_min,multispec_max)\n",
    "\n",
    "fig, axis = plt.subplots(1, 1, figsize=(10,4))\n",
    "for x,y in zip(thecapture.eo_indices(),thecapture.eo_band_names()):\n",
    "    axis.hist(im_aligned[:,:,x].ravel(), bins=512, range=theRange, \\\n",
    "              histtype=\"step\", label=y, color=theColors[y], linewidth=1.5)\n",
    "plt.title(\"Multispectral histogram (radiance)\")\n",
    "axis.legend()\n",
    "plt.show()\n",
    "\n",
    "if panchroCam:\n",
    "    eo_count = len(thecapture.eo_indices())\n",
    "    multispec_min = np.min(np.percentile(sharpened_stack[:,:,1:eo_count].flatten(),0.01))\n",
    "    multispec_max = np.max(np.percentile(sharpened_stack[:,:,1:eo_count].flatten(), 99.99))\n",
    "\n",
    "    theRange = (multispec_min,multispec_max)\n",
    "\n",
    "    fig, axis = plt.subplots(1, 1, figsize=(10,4))\n",
    "    for x,y in zip(thecapture.eo_indices(),thecapture.eo_band_names()):\n",
    "        axis.hist(sharpened_stack[:,:,x].ravel(), bins=512, range=theRange, \\\n",
    "                  histtype=\"step\", label=y, color=theColors[y], linewidth=1.5)\n",
    "    plt.title(\"Pan-sharpened multispectral histogram (radiance)\")\n",
    "    axis.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb819b",
   "metadata": {},
   "source": [
    "# Visualize Aligned Images\n",
    "Once the transformation has been found, it can be verified by compositing the aligned images to check alignment. The image 'stack' containing all bands can also be exported to a multi-band TIFF file for viewing in external software such as QGIS. Useful composites are a naturally colored RGB as well as color infrared, or CIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8626c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figsize=(30,23) # use this size for full-image-resolution display\n",
    "figsize=(16,13)   # use this size for export-sized display\n",
    "\n",
    "rgb_band_indices = [thecapture.band_names_lower().index('red'),\n",
    "                    thecapture.band_names_lower().index('green'),\n",
    "                    thecapture.band_names_lower().index('blue')]\n",
    "cir_band_indices = [thecapture.band_names_lower().index('nir'),\n",
    "                    thecapture.band_names_lower().index('red'),\n",
    "                    thecapture.band_names_lower().index('green')]\n",
    "\n",
    "# Create normalized stacks for viewing\n",
    "im_display = np.zeros((im_aligned.shape[0],im_aligned.shape[1],im_aligned.shape[2]), dtype=np.float32)\n",
    "im_min = np.percentile(im_aligned[:,:,rgb_band_indices].flatten(), 0.5)  # modify these percentiles to adjust contrast\n",
    "im_max = np.percentile(im_aligned[:,:,rgb_band_indices].flatten(), 99.5)  # for many images, 0.5 and 99.5 are good values\n",
    "\n",
    "if panchroCam:\n",
    "    im_display_sharp = np.zeros((sharpened_stack.shape[0],sharpened_stack.shape[1],sharpened_stack.shape[2]), dtype=np.float32 )\n",
    "    im_min_sharp = np.percentile(sharpened_stack[:,:,rgb_band_indices].flatten(), 0.5)  # modify these percentiles to adjust contrast\n",
    "    im_max_sharp = np.percentile(sharpened_stack[:,:,rgb_band_indices].flatten(), 99.5)  # for many images, 0.5 and 99.5 are good values\n",
    "\n",
    "\n",
    "# for rgb true color, we use the same min and max scaling across the 3 bands to \n",
    "# maintain the \"white balance\" of the calibrated image\n",
    "for i in rgb_band_indices:\n",
    "    im_display[:,:,i] =  imageutils.normalize(im_aligned[:,:,i], im_min, im_max)\n",
    "    if panchroCam: \n",
    "        im_display_sharp[:,:,i] = imageutils.normalize(sharpened_stack[:,:,i], im_min_sharp, im_max_sharp)\n",
    "\n",
    "rgb = im_display[:,:,rgb_band_indices]\n",
    "\n",
    "if panchroCam:\n",
    "    rgb_sharp = im_display_sharp[:,:,rgb_band_indices]\n",
    "\n",
    "nir_band = thecapture.band_names_lower().index('nir')\n",
    "red_band = thecapture.band_names_lower().index('red')\n",
    "\n",
    "ndvi = (im_aligned[:,:,nir_band] - im_aligned[:,:,red_band]) / (im_aligned[:,:,nir_band] + im_aligned[:,:,red_band])\n",
    "\n",
    "# for cir false color imagery, we normalize the NIR,R,G bands within themselves, which provides\n",
    "# the classical CIR rendering where plants are red and soil takes on a blue tint\n",
    "for i in cir_band_indices:\n",
    "    im_display[:,:,i] =  imageutils.normalize(im_aligned[:,:,i])\n",
    "\n",
    "cir = im_display[:,:,cir_band_indices]\n",
    "if panchroCam:\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "else:\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=figsize)\n",
    "ax1.set_title(\"Red-Green-Blue Composite\")\n",
    "ax1.imshow(rgb)\n",
    "if panchroCam:\n",
    "    ax2.set_title(\"Red-Green-Blue Composite (pan-sharpened)\")\n",
    "    ax2.imshow(rgb_sharp)\n",
    "\n",
    "fig, (ax3,ax4) = plt.subplots(1, 2, figsize=figsize)\n",
    "ax3.set_title(\"NDVI\")\n",
    "ax3.imshow(ndvi)\n",
    "ax4.set_title(\"Color Infrared (CIR) Composite\")\n",
    "ax4.imshow(cir)\n",
    "\n",
    "# set custom lims if you want to zoom in to image to see more detail \n",
    "# this is useful for comparing upsampled and pan-sharpened stacks \n",
    "# custom_xlim=(1500,2000)\n",
    "# custom_ylim=(2000,1500)\n",
    "# plt.setp([ax1,ax2,ax3,ax4], xlim=custom_xlim, ylim=custom_ylim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f71519",
   "metadata": {},
   "source": [
    "# Image Enhancement\n",
    "There are many techniques for image enhancement, but one which is commonly used to improve the visual sharpness of imagery is the unsharp mask. Here, we apply an unsharp mask to the RGB image to improve the visualization, and then apply a gamma curve to make the darkest areas brighter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if panchroCam:\n",
    "    rgb = rgb_sharp\n",
    "# Create an enhanced version of the RGB render using an unsharp mask\n",
    "gaussian_rgb = cv2.GaussianBlur(rgb, (9,9), 10.0)\n",
    "gaussian_rgb[gaussian_rgb<0] = 0\n",
    "gaussian_rgb[gaussian_rgb>1] = 1\n",
    "unsharp_rgb = cv2.addWeighted(rgb, 1.5, gaussian_rgb, -0.5, 0)\n",
    "unsharp_rgb[unsharp_rgb<0] = 0\n",
    "unsharp_rgb[unsharp_rgb>1] = 1\n",
    "\n",
    "# Apply a gamma correction to make the render appear closer to what our eyes would see\n",
    "gamma = 1.4\n",
    "gamma_corr_rgb = unsharp_rgb**(1.0/gamma)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "plt.imshow(gamma_corr_rgb, aspect='equal')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f6189",
   "metadata": {},
   "source": [
    "# Stack Export\n",
    "We can easily export the stacks into an image using the GDAL library (http://www.glal.org). Once exported, these image stacks can be opened in software such as QGIS and raster operations such as NDVI or NDRE computation can be done in that software. The stacks include geographic information. \n",
    "\n",
    "If you prefer, you may set `sort_by_wavelength` to `True` in the `save_capture_as_stack` function.\n",
    "\n",
    "Unless otherwise specified, this will save in your working folder, that is your `imageprocessing` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d716be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set output name to unique capture ID, e.g. FWoNSvgDNBX63Xv378qs\n",
    "outputName = thecapture.uuid\n",
    "\n",
    "st = time.time()\n",
    "if panchroCam:\n",
    "    # in this example, we can export both a pan-sharpened stack and an upsampled stack\n",
    "    # so you can compare them in GIS. In practice, you would typically only output the pansharpened stack \n",
    "    thecapture.save_capture_as_stack(outputName+\"-pansharpened.tif\", sort_by_wavelength=True, pansharpen=True)\n",
    "    thecapture.save_capture_as_stack(outputName+\"-upsampled.tif\", sort_by_wavelength=True, pansharpen=False)\n",
    "else:\n",
    "    thecapture.save_capture_as_stack(outputName+\"-noPanels.tif\", sort_by_wavelength=True)\n",
    "\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print(\"Time to save stacks:\", int(elapsed_time), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9cd64",
   "metadata": {},
   "source": [
    "# NDVI Computation\n",
    "For raw index computation on single images, the `numpy` package provides a simple way to do math and simple visualization on images. Below, we compute and visualize an image histogram, and then use that to pick a color map range for visualizing the NDVI of an image.\n",
    "\n",
    "## Plant Classification\n",
    "After computing the NDVI and prior to displaying it, we use a very rudimentary method for focusing on the plants and removing the soil and shadow information from our images and histograms. Below, we remove non-plant pixels by setting to zero any pixels in the image where the NIR reflectance is less than 20%. This helps to ensure that the NDVI and NDRE histograms aren't skewed substantially by soil noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a572a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from micasense import plotutils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nir_band = thecapture.band_names_lower().index('nir')\n",
    "red_band = thecapture.band_names_lower().index('red')\n",
    "\n",
    "thelayer = im_aligned\n",
    "if panchroCam:\n",
    "    thelayer = sharpened_stack\n",
    "np.seterr(divide='ignore', invalid='ignore') # ignore divide by zero errors in the index calculation\n",
    "\n",
    "# Compute Normalized Difference Vegetation Index (NDVI) from the NIR(3) and RED (2) bands\n",
    "ndvi = (thelayer[:,:,nir_band] - thelayer[:,:,red_band]) / (thelayer[:,:,nir_band] + thelayer[:,:,red_band])\n",
    "print(\"Image type:\",img_type)\n",
    "\n",
    "# remove shadowed areas (mask pixels with NIR reflectance < 20%))\n",
    "# this does not seem to work on panchro stacks \n",
    "if img_type == 'reflectance':\n",
    "    ndvi = np.ma.masked_where(thelayer[:,:,nir_band] < 0.20, ndvi) \n",
    "elif img_type == 'radiance':\n",
    "    lower_pct_radiance = np.percentile(thelayer[:,:,nir_band],  10.0)\n",
    "    ndvi = np.ma.masked_where(thelayer[:,:,nir_band] < lower_pct_radiance, ndvi) \n",
    "# Compute and display a histogram\n",
    "# ndvi_hist_min = np.min(ndvi) \n",
    "# ndvi_hist_max = np.max(ndvi) \n",
    "ndvi_hist_min = np.min(np.percentile(ndvi,0.5))\n",
    "ndvi_hist_max = np.max(np.percentile(ndvi,99.5))\n",
    "fig, axis = plt.subplots(1, 1, figsize=(10,4))\n",
    "axis.hist(ndvi.ravel(), bins=512, range=(ndvi_hist_min, ndvi_hist_max))\n",
    "plt.title(\"NDVI Histogram\")\n",
    "plt.show()\n",
    "\n",
    "min_display_ndvi = 0.45 # further mask soil by removing low-ndvi values\n",
    "#min_display_ndvi = np.percentile(ndvi.flatten(),  5.0)  # modify with these percentilse to adjust contrast\n",
    "max_display_ndvi = np.percentile(ndvi.flatten(), 99.5)  # for many images, 0.5 and 99.5 are good values\n",
    "masked_ndvi = np.ma.masked_where(ndvi < min_display_ndvi, ndvi)\n",
    "\n",
    "#reduce the figure size to account for colorbar\n",
    "figsize=np.asarray(figsize) - np.array([3,2])\n",
    "\n",
    "#plot NDVI over an RGB basemap, with a colorbar showing the NDVI scale\n",
    "fig, axis = plotutils.plot_overlay_withcolorbar(gamma_corr_rgb, \n",
    "                                    masked_ndvi, \n",
    "                                    figsize = (14,7), \n",
    "                                    title = 'NDVI filtered to only plants over RGB base layer',\n",
    "                                    vmin = min_display_ndvi,\n",
    "                                    vmax = max_display_ndvi)\n",
    "fig.savefig(thecapture.uuid+'_ndvi_over_rgb.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4cf19",
   "metadata": {},
   "source": [
    "# NDRE Computation\n",
    "In the same manner, we can compute, filter, and display another index useful for MicaSense cameras, the Normalized Difference Red Edge (NDRE) index. We also filter out shadows and soil to ensure our display focuses only on the plant health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Normalized Difference Red Edge Index from the NIR(3) and RedEdge(4) bands\n",
    "rededge_band = thecapture.band_names_lower().index('red edge')\n",
    "ndre = (thelayer[:,:,nir_band] - thelayer[:,:,rededge_band]) / (thelayer[:,:,nir_band] + thelayer[:,:,rededge_band])\n",
    "\n",
    "# Mask areas with shadows and low NDVI to remove soil\n",
    "masked_ndre = np.ma.masked_where(ndvi < min_display_ndvi, ndre)\n",
    "\n",
    "# Compute a histogram\n",
    "ndre_hist_min = np.min(np.percentile(masked_ndre,0.5))\n",
    "ndre_hist_max = np.max(np.percentile(masked_ndre,99.5))\n",
    "fig, axis = plt.subplots(1, 1, figsize=(10,4))\n",
    "axis.hist(masked_ndre.ravel(), bins=512, range=(ndre_hist_min, ndre_hist_max))\n",
    "plt.title(\"NDRE Histogram (filtered to only plants)\")\n",
    "plt.show()\n",
    "\n",
    "min_display_ndre = np.percentile(masked_ndre, 5)\n",
    "max_display_ndre = np.percentile(masked_ndre, 99.5)\n",
    "\n",
    "fig, axis = plotutils.plot_overlay_withcolorbar(gamma_corr_rgb, \n",
    "                                    masked_ndre, \n",
    "                                    figsize=(14,7), \n",
    "                                    title='NDRE filtered to only plants over RGB base layer',\n",
    "                                    vmin=min_display_ndre,vmax=max_display_ndre)\n",
    "fig.savefig(thecapture.uuid+'_ndre_over_rgb.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898cc28a",
   "metadata": {},
   "source": [
    "# Thermal Imagery\n",
    "If our image is from an Altum or Altum-PT and includes a thermal band, we can display the re-sampled and aligned thermal data over the RGB data to maintain the context of the thermal information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(thecapture.lw_indices()) > 0:\n",
    "    lwir_band = thecapture.band_names_lower().index('lwir')\n",
    "\n",
    "    # by default we don't mask the thermal, since it's native resolution is much lower than the MS\n",
    "    if panchroCam:\n",
    "        masked_thermal = sharpened_stack[:,:,lwir_band]\n",
    "    else:\n",
    "        masked_thermal = im_aligned[:,:,lwir_band]\n",
    "    # Alternatively we can mask the thermal only to plants here, which is useful for large contiguous areas\n",
    "    # masked_thermal = np.ma.masked_where(ndvi < 0.45, im_aligned[:,:,5])\n",
    "\n",
    "\n",
    "    # Compute a histogram\n",
    "    fig, axis = plt.subplots(1, 1, figsize=(10,4))\n",
    "    axis.hist(masked_thermal.ravel(), bins=512, range=(np.min(np.percentile(masked_thermal,1)), np.max(np.percentile(masked_thermal,99))))\n",
    "    plt.title(\"Thermal Histogram\")\n",
    "    plt.show()\n",
    "\n",
    "    min_display_therm = np.percentile(masked_thermal, 1)\n",
    "    max_display_therm = np.percentile(masked_thermal, 99)\n",
    "\n",
    "    fig, axis = plotutils.plot_overlay_withcolorbar(gamma_corr_rgb,\n",
    "                                        masked_thermal, \n",
    "                                        figsize=(14,7), \n",
    "                                        title='Temperature over True Color',\n",
    "                                        vmin=min_display_therm,vmax=max_display_therm,\n",
    "                                        overlay_alpha=0.25,\n",
    "                                        overlay_colormap='jet',\n",
    "                                        overlay_steps=16,\n",
    "                                        display_contours=True,\n",
    "                                        contour_steps=16,\n",
    "                                        contour_alpha=.4,\n",
    "                                        contour_fmt=\"%.0fC\")\n",
    "    fig.savefig(thecapture.uuid+'_thermal_over_rgb.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b29ec",
   "metadata": {},
   "source": [
    "# Red vs NIR Reflectance\n",
    "Finally, we show a classic agricultural remote sensing output in the tassled cap plot. This plot can be useful for visualizing row crops and plots the Red Reflectance channel on the X-axis against the NIR reflectance channel on the Y-axis. This plot also clearly shows the line of the soil in that space. The tassled cap view isn't very useful for this arid data set; however, we can see the \"badge of trees\" of high NIR reflectance and relatively low red reflectance. This provides an example of one of the uses of aligned images for single capture analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0250872",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_band = red_band\n",
    "y_band = nir_band\n",
    "x_max = np.max(np.percentile(im_aligned[:,:,x_band],99.99))\n",
    "y_max = np.max(np.percentile(im_aligned[:,:,y_band],99.99))\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.hexbin(im_aligned[:,:,x_band],im_aligned[:,:,y_band],gridsize=640,bins='log',extent=(0,x_max,0,y_max))\n",
    "ax = fig.gca()\n",
    "ax.set_xlim([0,x_max])\n",
    "ax.set_ylim([0,y_max])\n",
    "plt.xlabel(\"{} Reflectance\".format(thecapture.band_names()[x_band]))\n",
    "plt.ylabel(\"{} Reflectance\".format(thecapture.band_names()[y_band]))\n",
    "plt.show()\n",
    "\n",
    "if panchroCam:\n",
    "    x_band = red_band\n",
    "    y_band = nir_band\n",
    "    x_max = np.max(np.percentile(sharpened_stack[:,:,x_band],99.99))\n",
    "    y_max = np.max(np.percentile(sharpened_stack[:,:,y_band],99.99))\n",
    "\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    plt.hexbin(sharpened_stack[:,:,x_band],sharpened_stack[:,:,y_band],gridsize=640,bins='log',extent=(0,x_max,0,y_max))\n",
    "    ax = fig.gca()\n",
    "    ax.set_xlim([0,x_max])\n",
    "    ax.set_ylim([0,y_max])\n",
    "    plt.xlabel(\"{} Reflectance (pan-sharpened)\".format(thecapture.band_names()[x_band]))\n",
    "    plt.ylabel(\"{} Reflectance (pan-sharpened)\".format(thecapture.band_names()[y_band]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2d106",
   "metadata": {},
   "source": [
    "# Print warp_matrices for usage elsewhere, such as Batch Processing\n",
    "Lastly, we output the `warp_matrices` that we got for this image stack for usage elsewhere. Currently, these can be used in the Batch Processing.ipynb notebook to save reflectance-compensated stacks of images to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if panchroCam:\n",
    "    print(warp_matrices_SIFT)\n",
    "else:\n",
    "    print(warp_matrices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:micasense] *",
   "language": "python",
   "name": "conda-env-micasense-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
